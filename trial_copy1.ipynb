{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path, mkdir, environ\n",
    "import sys\n",
    "sys.path.append(path.join(path.dirname(__file__), 'src', 'NeuralFingerPrint'))\n",
    "sys.path.append(path.dirname(__file__))\n",
    "from time import time\n",
    "import torch\n",
    "from torch import save, load\n",
    "from torch.utils import data\n",
    "from src import attention_model, drug_drug, setting, my_data, logger, device2\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pdb\n",
    "import shap\n",
    "import pickle\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import wandb\n",
    "import data_utils\n",
    "import concurrent.futures\n",
    "import random\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "random_seed = 913\n",
    "\n",
    "def set_seed(seed=random_seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_final_index():\n",
    "\n",
    "    ## get the index of synergy score database\n",
    "    if not setting.update_final_index and path.exists(setting.final_index):\n",
    "        final_index = pd.read_csv(setting.final_index, header=None)[0]\n",
    "    else:\n",
    "        final_index = my_data.SynergyDataReader.get_final_index()\n",
    "    return final_index\n",
    "\n",
    "def prepare_data():\n",
    "\n",
    "    if not setting.update_xy:\n",
    "        assert (path.exists(setting.old_x) and path.exists(setting.old_y)), \"Data need to be downloaded from zenodo follow instruction in README\"\n",
    "        X = np.load(setting.old_x)\n",
    "        with open(setting.old_x_lengths, 'rb') as old_x_lengths:\n",
    "            drug_features_length, cellline_features_length = pickle.load(old_x_lengths)\n",
    "        with open(setting.old_y, 'rb') as old_y:\n",
    "            Y = pickle.load(old_y)\n",
    "    else:\n",
    "        X, drug_features_length, cellline_features_length = \\\n",
    "            my_data.SamplesDataLoader.Raw_X_features_prep(methods='flexible_attn')\n",
    "        np.save(setting.old_x, X)\n",
    "        with open(setting.old_x_lengths, 'wb+') as old_x_lengths:\n",
    "            pickle.dump((drug_features_length,cellline_features_length), old_x_lengths)\n",
    "\n",
    "        Y = my_data.SamplesDataLoader.Y_features_prep()\n",
    "        with open(setting.old_y, 'wb+') as old_y:\n",
    "            pickle.dump(Y, old_y)\n",
    "    return X, Y, drug_features_length, cellline_features_length\n",
    "\n",
    "\n",
    "def prepare_model(reorder_tensor, entrez_set):\n",
    "\n",
    "    ### prepare two models\n",
    "    ### drug_model: the one used for training\n",
    "    ### best_drug_mode;: the one used for same the best model\n",
    "\n",
    "    final_mask = None\n",
    "    drug_model = attention_model.get_multi_models(reorder_tensor.get_reordered_slice_indices(), input_masks=final_mask,\n",
    "                                                  drugs_on_the_side=False)\n",
    "    best_drug_model = attention_model.get_multi_models(reorder_tensor.get_reordered_slice_indices(),\n",
    "                                                       input_masks=final_mask, drugs_on_the_side=False)\n",
    "    for n, m in drug_model.named_modules():\n",
    "        if n == \"out\":\n",
    "            m.register_forward_hook(drug_drug.input_hook)\n",
    "    for best_n, best_m in best_drug_model.named_modules():\n",
    "        if best_n == \"out\":\n",
    "            best_m.register_forward_hook(drug_drug.input_hook)\n",
    "    drug_model = drug_model.to(device2)\n",
    "    best_drug_model = best_drug_model.to(device2)\n",
    "    if USE_wandb:\n",
    "        wandb.watch(drug_model, log=\"all\")\n",
    "    return drug_model, best_drug_model\n",
    "\n",
    "\n",
    "def persist_data_as_data_point_file(local_X, final_index_for_X):\n",
    "\n",
    "    ### prepare files for dataloader\n",
    "    for i, combin_drug_feature_array in enumerate(local_X):\n",
    "        if setting.unit_test:\n",
    "            if i <= 501:  # and not path.exists(path.join('datas', str(final_index_for_X.iloc[i]) + '.pt')):\n",
    "                save(combin_drug_feature_array, path.join(setting.data_folder, str(final_index_for_X.iloc[i]) + '.pt'))\n",
    "        else:\n",
    "            if setting.update_features or not path.exists(\n",
    "                    path.join(setting.data_folder, str(final_index_for_X.iloc[i]) + '.pt')):\n",
    "                save(combin_drug_feature_array, path.join(setting.data_folder, str(final_index_for_X.iloc[i]) + '.pt'))\n",
    "\n",
    "\n",
    "def prepare_splitted_dataset(partition, labels):\n",
    "\n",
    "    ### prepare train, test, evaluation data generator\n",
    "\n",
    "    logger.debug(\"Preparing datasets ... \")\n",
    "    # training_set = my_data.MyDataset(partition['train'] + partition['eval1'] + partition['eval2'], labels)\n",
    "    training_set = my_data.MyDataset(partition['train'], labels)\n",
    "    train_params = {'batch_size': setting.batch_size,\n",
    "                    'shuffle': True}\n",
    "    training_generator = data.DataLoader(training_set, **train_params)\n",
    "\n",
    "    eval_train_set = my_data.MyDataset(partition['train'] + partition['eval1'] + partition['eval2'], labels)\n",
    "    training_index_list = partition['train'] + partition['eval1'] + partition['eval2']\n",
    "    logger.debug(\"Training data length: {!r}\".format(len(training_index_list)))\n",
    "    eval_train_params = {'batch_size': setting.batch_size,\n",
    "                         'shuffle': False}\n",
    "    eval_train_generator = data.DataLoader(eval_train_set, **eval_train_params)\n",
    "\n",
    "    # validation_set = my_data.MyDataset(partition['test1'], labels)\n",
    "    validation_set = my_data.MyDataset(partition['eval1'], labels)\n",
    "    eval_params = {'batch_size': len(partition['test1'])//4,\n",
    "                   'shuffle': False}\n",
    "    validation_generator = data.DataLoader(validation_set, **eval_params)\n",
    "\n",
    "    test_set = my_data.MyDataset(partition['test1'], labels)\n",
    "    test_index_list = partition['test1']\n",
    "    logger.debug(\"Test data length: {!r}\".format(len(test_index_list)))\n",
    "    pickle.dump(test_index_list, open(\"test_index_list\", \"wb+\"))\n",
    "    test_params = {'batch_size': len(test_index_list) // 4,\n",
    "                   'shuffle': False}\n",
    "    test_generator = data.DataLoader(test_set, **test_params)\n",
    "\n",
    "    all_index_list = partition['train'][:len(partition['train']) // 2] + partition['eval1'] + partition['test1']\n",
    "    all_set = my_data.MyDataset(all_index_list, labels)\n",
    "    logger.debug(\"All data length: {!r}\".format(len(set(all_index_list))))\n",
    "    pickle.dump(all_index_list, open(\"all_index_list\", \"wb+\"))\n",
    "    all_set_params = {'batch_size': len(all_index_list) // 8,\n",
    "                      'shuffle': False}\n",
    "    all_data_generator = data.DataLoader(all_set, **all_set_params)\n",
    "    ### generate all the data in one iteration because the batch size is bigger than all_data_generator\n",
    "    all_set_params_total = {'batch_size': len(all_index_list),\n",
    "                            'shuffle': False}\n",
    "    all_data_generator_total = data.DataLoader(all_set, **all_set_params_total)\n",
    "    return training_generator, eval_train_generator, validation_generator, test_generator, all_data_generator, all_data_generator_total\n",
    "\n",
    "def run():\n",
    "\n",
    "    final_index = get_final_index()\n",
    "    ## get genes\n",
    "    entrez_set = my_data.GenesDataReader.get_gene_entrez_set()\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    logger.debug(\"Getting features and synergy scores ...\")\n",
    "\n",
    "    X, Y, drug_features_length, cellline_features_length = prepare_data()\n",
    "\n",
    "    logger.debug(\"Preparing models\")\n",
    "    slice_indices = drug_features_length + drug_features_length + cellline_features_length\n",
    "    reorder_tensor = drug_drug.reorganize_tensor(slice_indices, setting.arrangement, 2)\n",
    "    logger.debug(\"the layout of all features is {!r}\".format(reorder_tensor.get_reordered_slice_indices()))\n",
    "    set_seed()\n",
    "    drug_model, best_drug_model = prepare_model(reorder_tensor, entrez_set)\n",
    "\n",
    "    optimizer = torch.optim.Adam(drug_model.parameters(), lr=setting.start_lr, weight_decay=setting.lr_decay,\n",
    "                                 betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min = 1e-7)\n",
    "\n",
    "    # define variables used in testing and shap analysis\n",
    "    test_generator = None\n",
    "    all_data_generator_total = None\n",
    "    all_data_generator = None\n",
    "    test_index_list = None\n",
    "    best_cv_pearson_score = 0\n",
    "    partition = None\n",
    "\n",
    "    split_func = my_data.DataPreprocessor.reg_train_eval_test_split\n",
    "    logger.debug(\"Spliting data ...\")\n",
    "\n",
    "    for train_index, test_index, test_index_2, evaluation_index, evaluation_index_2 in split_func(fold='fold', test_fold = 4):\n",
    "\n",
    "        local_X = X[np.concatenate((train_index, test_index, test_index_2, evaluation_index, evaluation_index_2))]\n",
    "        final_index_for_X = final_index.iloc[np.concatenate((train_index, test_index,\n",
    "                                                             test_index_2, evaluation_index, evaluation_index_2))]\n",
    "\n",
    "        ori_Y = Y\n",
    "        std_scaler.fit(Y[train_index])\n",
    "        if setting.y_transform:\n",
    "            Y = std_scaler.transform(Y) * 100\n",
    "\n",
    "        persist_data_as_data_point_file(local_X, final_index_for_X)\n",
    "\n",
    "        partition = {'train': list(final_index.iloc[train_index]),\n",
    "                     'test1': list(final_index.iloc[test_index]), 'test2': list(final_index.iloc[test_index_2]),\n",
    "                     'eval1': list(final_index.iloc[evaluation_index]),\n",
    "                     'eval2': list(final_index.iloc[evaluation_index_2])}\n",
    "\n",
    "        labels = {key: value for key, value in zip(list(final_index),\n",
    "                                                   list(Y.reshape(-1)))}\n",
    "        ori_labels = {key: value for key, value in zip(list(final_index),\n",
    "                                                   list(ori_Y.reshape(-1)))}\n",
    "        save(ori_labels, setting.y_labels_file)\n",
    "\n",
    "        training_generator, eval_train_generator, validation_generator, test_generator, \\\n",
    "        all_data_generator, all_data_generator_total = prepare_splitted_dataset(partition, labels)\n",
    "        test_index_list = partition['test1']\n",
    "\n",
    "        logger.debug(\"Start training\")\n",
    "        set_seed()\n",
    "\n",
    "        for epoch in range(setting.n_epochs):\n",
    "\n",
    "            drug_model.train()\n",
    "            start = time()\n",
    "            cur_epoch_train_loss = []\n",
    "            train_total_loss = 0\n",
    "            train_i = 0\n",
    "            all_preds = []\n",
    "            all_ys = []\n",
    "\n",
    "            training_iter = iter(training_generator)\n",
    "\n",
    "\n",
    "            # Training\n",
    "            for (cur_local_batch, cur_smiles_a, cur_smiles_b), cur_local_labels in training_iter:\n",
    "\n",
    "                train_i += 1\n",
    "                # Transfer to GPU\n",
    "                if epoch == 0 and train_i == 1:\n",
    "                    print('--------------------------------cur local labels---------------------------------')\n",
    "                    print(cur_local_labels)\n",
    "                    print('--------------------------------cur local labels---------------------------------')\n",
    "                pre_local_batch = cur_local_batch\n",
    "                pre_local_labels = cur_local_labels\n",
    "                local_labels_on_cpu = np.array(pre_local_labels).reshape(-1)\n",
    "                sample_size = local_labels_on_cpu.shape[-1]\n",
    "                local_labels_on_cpu = local_labels_on_cpu[:sample_size]\n",
    "                local_batch, local_labels = pre_local_batch.float().to(device2), pre_local_labels.float().to(device2)\n",
    "                local_batch = local_batch.contiguous().view(-1, 1, sum(slice_indices) + setting.single_repsonse_feature_length)\n",
    "                reorder_tensor.load_raw_tensor(local_batch)\n",
    "                local_batch = reorder_tensor.get_reordered_narrow_tensor()\n",
    "\n",
    "                preds = drug_model(*local_batch)\n",
    "                preds = preds.contiguous().view(-1)\n",
    "                ys = local_labels.contiguous().view(-1)\n",
    "                optimizer.zero_grad()\n",
    "                assert preds.size(-1) == ys.size(-1)\n",
    "                loss = F.mse_loss(preds, ys)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                prediction_on_cpu = preds.detach().cpu().numpy().reshape(-1)\n",
    "                # mean_prediction_on_cpu = np.mean([prediction_on_cpu[:sample_size],\n",
    "                #                                   prediction_on_cpu[sample_size:]], axis=0)\n",
    "                mean_prediction_on_cpu = prediction_on_cpu[:sample_size]\n",
    "                if setting.y_transform:\n",
    "                    local_labels_on_cpu, mean_prediction_on_cpu = \\\n",
    "                        std_scaler.inverse_transform(local_labels_on_cpu.reshape(-1, 1) / 100), \\\n",
    "                        std_scaler.inverse_transform(mean_prediction_on_cpu.reshape(-1, 1) / 100)\n",
    "                all_preds.append(mean_prediction_on_cpu)\n",
    "                all_ys.append(local_labels_on_cpu)\n",
    "                pre_local_batch = cur_local_batch\n",
    "                pre_local_labels = cur_local_labels\n",
    "\n",
    "                train_total_loss += loss.item()\n",
    "\n",
    "                n_iter = 50\n",
    "                if train_i % n_iter == 0:\n",
    "                    sample_size = len(train_index) + 2* len(evaluation_index)\n",
    "                    p = int(100 * train_i * setting.batch_size / sample_size)\n",
    "                    avg_loss = train_total_loss / n_iter\n",
    "                    if setting.y_transform:\n",
    "                        avg_loss = std_scaler.inverse_transform(np.array(avg_loss/100).reshape(-1,1)).reshape(-1)[0]\n",
    "                    logger.debug(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" % \\\n",
    "                          ((time() - start) // 60, epoch, \"\".join('#' * (p // 5)),\n",
    "                           \"\".join(' ' * (20 - (p // 5))), p, avg_loss))\n",
    "                    train_total_loss = 0\n",
    "                    cur_epoch_train_loss.append(avg_loss)\n",
    "\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_ys = np.concatenate(all_ys)\n",
    "\n",
    "            assert len(all_preds) == len(all_ys), \"predictions and labels are in different length\"\n",
    "            val_train_loss = mean_squared_error(all_preds, all_ys)\n",
    "            val_train_pearson = pearsonr(all_preds.reshape(-1), all_ys.reshape(-1))[0]\n",
    "            val_train_spearman = spearmanr(all_preds.reshape(-1), all_ys.reshape(-1))[0]\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            ### Evaluation\n",
    "            val_train_i = 0\n",
    "            save_data_num = 0\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "\n",
    "                drug_model.eval()\n",
    "                all_preds = []\n",
    "                all_ys = []\n",
    "\n",
    "                all_preds = []\n",
    "                all_ys = []\n",
    "                val_i = 0\n",
    "\n",
    "                validation_iter = iter(validation_generator)\n",
    "\n",
    "                for (cur_local_batch, cur_smiles_a, cur_smiles_b), cur_local_labels in validation_iter:\n",
    "\n",
    "                    val_i += 1\n",
    "                    pre_local_batch = cur_local_batch\n",
    "                    pre_local_labels = cur_local_labels\n",
    "                    local_labels_on_cpu = np.array(pre_local_labels).reshape(-1)\n",
    "                    sample_size = local_labels_on_cpu.shape[-1]\n",
    "                    local_labels_on_cpu = local_labels_on_cpu[:sample_size]\n",
    "                    # Transfer to GPU\n",
    "                    local_batch, local_labels = pre_local_batch.float().to(device2), pre_local_labels.float().to(device2)\n",
    "                    # local_batch = local_batch[:,:sum(slice_indices) + setting.single_repsonse_feature_length]\n",
    "                    reorder_tensor.load_raw_tensor(local_batch.contiguous().view(-1, 1, sum(slice_indices) + setting.single_repsonse_feature_length))\n",
    "                    local_batch = reorder_tensor.get_reordered_narrow_tensor()\n",
    "                    \n",
    "                    preds = drug_model(*local_batch)\n",
    "                    preds = preds.contiguous().view(-1)\n",
    "                    assert preds.size(-1) == local_labels.size(-1)\n",
    "                    prediction_on_cpu = preds.cpu().numpy().reshape(-1)\n",
    "                    \n",
    "                    mean_prediction_on_cpu = prediction_on_cpu[:sample_size]\n",
    "                    if setting.y_transform:\n",
    "                        local_labels_on_cpu, mean_prediction_on_cpu = \\\n",
    "                            std_scaler.inverse_transform(local_labels_on_cpu.reshape(-1,1) / 100), \\\n",
    "                            std_scaler.inverse_transform(mean_prediction_on_cpu.reshape(-1,1) / 100)\n",
    "                    all_preds.append(mean_prediction_on_cpu)\n",
    "                    all_ys.append(local_labels_on_cpu)\n",
    "                    pre_local_batch = cur_local_batch\n",
    "                    pre_local_labels = cur_local_labels\n",
    "\n",
    "                all_preds = np.concatenate(all_preds)\n",
    "                all_ys = np.concatenate(all_ys)\n",
    "\n",
    "                assert len(all_preds) == len(all_ys), \"predictions and labels are in different length\"\n",
    "                val_loss = mean_squared_error(all_preds, all_ys)\n",
    "                val_pearson = pearsonr(all_preds.reshape(-1), all_ys.reshape(-1))[0]\n",
    "                val_spearman = spearmanr(all_preds.reshape(-1), all_ys.reshape(-1))[0]\n",
    "\n",
    "                if best_cv_pearson_score < val_pearson:\n",
    "                    logger.debug('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "                    logger.debug('saved a model, sample size {0!r}'.format(len(all_preds)))\n",
    "                    logger.debug('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "                    best_cv_pearson_score = val_pearson\n",
    "                    best_drug_model.load_state_dict(drug_model.state_dict())\n",
    "\n",
    "            logger.debug(\n",
    "                \"Training mse is {0}, Training pearson correlation is {1!r}, Training spearman correlation is {2!r}\"\n",
    "                    .format(np.mean(val_train_loss), val_train_pearson, val_train_spearman))\n",
    "\n",
    "            logger.debug(\n",
    "                \"Validation mse is {0}, Validation pearson correlation is {1!r}, Validation spearman correlation is {2!r}\"\n",
    "                    .format(np.mean(val_loss), val_pearson, val_spearman))\n",
    "\n",
    "            if USE_wandb:\n",
    "                wandb.log({\"Training mse\": np.mean(val_train_loss), \"Training pearson correlation\": val_train_pearson,\n",
    "                           \"Training spearman correlation\": val_train_spearman}, step=epoch)\n",
    "                wandb.log({\"Validation mse\": np.mean(val_loss), \"Validation pearson correlation\": val_pearson,\n",
    "                           \"Validation spearman correlation\": val_spearman}, step=epoch)\n",
    "\n",
    "    ### Testing\n",
    "\n",
    "    if setting.load_old_model:\n",
    "        best_drug_model.load_state_dict(load(setting.old_model_path).state_dict())\n",
    "\n",
    "    test_i = 0\n",
    "    save_data_num = 0\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        best_drug_model.eval()\n",
    "        all_preds = []\n",
    "        all_ys = []\n",
    "\n",
    "        test_iter = iter(test_generator)\n",
    "        \n",
    "\n",
    "        for (cur_local_batch, cur_smiles_a, cur_smiles_b), cur_local_labels in test_iter:\n",
    "            # Transfer to GPU\n",
    "            test_i += 1\n",
    "            pre_local_batch = cur_local_batch\n",
    "            pre_local_labels = cur_local_labels\n",
    "            local_labels_on_cpu = np.array(pre_local_labels).reshape(-1)\n",
    "            sample_size = local_labels_on_cpu.shape[-1]\n",
    "            local_labels_on_cpu = local_labels_on_cpu[:sample_size]\n",
    "            local_batch, local_labels = pre_local_batch.float().to(device2), pre_local_labels.float().to(device2)\n",
    "            # local_batch = local_batch[:,:sum(slice_indices) + setting.single_repsonse_feature_length]\n",
    "            reorder_tensor.load_raw_tensor(local_batch.contiguous().view(-1, 1, sum(slice_indices) + setting.single_repsonse_feature_length))\n",
    "            local_batch = reorder_tensor.get_reordered_narrow_tensor()\n",
    "            \n",
    "            preds = best_drug_model(*local_batch)\n",
    "            preds = preds.contiguous().view(-1)\n",
    "            cur_test_start_index = len(test_index_list) // 4 * (test_i-1)\n",
    "            cur_test_stop_index = min(len(test_index_list) // 4 * (test_i), len(test_index_list))\n",
    "            for n, m in best_drug_model.named_modules():\n",
    "                if n == \"out\":\n",
    "                    catoutput = m._value_hook[0]\n",
    "            for i, test_combination in enumerate(test_index_list[cur_test_start_index: cur_test_stop_index]):\n",
    "                if not path.exists(\"test_\" + setting.catoutput_output_type + \"_datas\"):\n",
    "                    mkdir(\"test_\" + setting.catoutput_output_type + \"_datas\")\n",
    "                save(catoutput.narrow_copy(0, i, 1), path.join(\"test_\" + setting.catoutput_output_type + \"_datas\",\n",
    "                                                               str(test_combination) + '.pt'))\n",
    "                save_data_num += 1\n",
    "            assert preds.size(-1) == local_labels.size(-1)\n",
    "            prediction_on_cpu = preds.cpu().numpy().reshape(-1)\n",
    "            mean_prediction_on_cpu = prediction_on_cpu\n",
    "            if setting.y_transform:\n",
    "                local_labels_on_cpu, mean_prediction_on_cpu = \\\n",
    "                    std_scaler.inverse_transform(local_labels_on_cpu.reshape(-1, 1) / 100), \\\n",
    "                    std_scaler.inverse_transform(prediction_on_cpu.reshape(-1, 1) / 100)\n",
    "            all_preds.append(mean_prediction_on_cpu)\n",
    "            all_ys.append(local_labels_on_cpu)\n",
    "            pre_local_batch = cur_local_batch\n",
    "            pre_local_labels = cur_local_labels\n",
    "\n",
    "\n",
    "        logger.debug(\"saved {!r} data for testing dataset\".format(save_data_num))\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_ys = np.concatenate(all_ys)\n",
    "        assert len(all_preds) == len(all_ys), \"predictions and labels are in different length\"\n",
    "        sample_size = len(all_preds)\n",
    "        mean_prediction = np.mean([all_preds[:sample_size],\n",
    "                                          all_preds[:sample_size]], axis=0)\n",
    "        mean_y = np.mean([all_ys[:sample_size],\n",
    "                          all_ys[:sample_size]], axis=0)\n",
    "\n",
    "        test_loss = mean_squared_error(mean_prediction, mean_y)\n",
    "        test_pearson = pearsonr(mean_y.reshape(-1), mean_prediction.reshape(-1))[0]\n",
    "        test_spearman = spearmanr(mean_y.reshape(-1), mean_prediction.reshape(-1))[0]\n",
    "        if not path.exists('prediction'):\n",
    "            mkdir('prediction')\n",
    "        save(np.concatenate((np.array(test_index_list[:sample_size]).reshape(-1,1), mean_prediction.reshape(-1, 1), mean_y.reshape(-1, 1)), axis=1),\n",
    "             \"prediction/prediction_\" + setting.catoutput_output_type + \"_testing\")\n",
    "\n",
    "    logger.debug(\"Testing mse is {0}, Testing pearson correlation is {1!r}, Testing spearman correlation is {2!r}\".format(np.mean(test_loss), test_pearson, test_spearman))\n",
    "\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    USE_wandb = False\n",
    "    if USE_wandb:\n",
    "        wandb.init(project=\"Drug combination alpha\",\n",
    "                   name=setting.run_dir.rsplit('/', 1)[1] + '_' + setting.data_specific[:15] + '_' + str(random_seed),\n",
    "                   notes=setting.data_specific)\n",
    "    else:\n",
    "        environ[\"WANDB_MODE\"] = \"dryrun\"\n",
    "\n",
    "    try:\n",
    "        run()\n",
    "        logger.debug(\"new directory %s\" % setting.run_dir)\n",
    "\n",
    "    except:\n",
    "\n",
    "        import shutil\n",
    "\n",
    "        #shutil.rmtree(setting.run_dir)\n",
    "        logger.debug(\"clean directory %s\" % setting.run_dir)\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3c0e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.array([1,2,3])\n",
    "print(x1+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653915a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d9e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d250abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
